{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 다운"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Song Lyrics 데이터 다운"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ wget https://aiffelstaticprd.blob.core.windows.net/media/documents/song_lyrics.zip\n",
    "$ unzip song_lyrics.zip -d ~/aiffel/lyricist/data/lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['\\ufeffbaby It was all a dream', 'I used to read Word Up magazine', 'Salt n Pepa and Heavy D up in the limousine']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# txt 파일을 모두 읽어서 raw_corpus 에 담기\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿baby It was all a dream\n",
      "I used to read Word Up magazine\n",
      "Salt n Pepa and Heavy D up in the limousine\n",
      "Hangin pictures on my wall\n",
      "Every Saturday Rap Attack Mr Magic Marley Marl\n",
      "I let my tape rock til my tape popped\n",
      "Smokin weed and Bambu sippin on Private Stock\n",
      "Way back when I had the red and black lumberjack\n",
      "With the hat to match\n",
      "Remember Rappin Duke duhha duhha\n",
      "You never thought that hip hop would take it this far\n",
      "Now Im in the limelight cause I rhyme tight\n",
      "Time to get paid blow up like the World Trade\n",
      "Born sinner the opposite of a winner\n",
      "Remember when I used to eat sardines for dinner\n",
      "Peace to Ron G Brucey B Kid Capri\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   \n",
    "    if sentence[-1] == \":\": continue  \n",
    "\n",
    "    if idx > 15: break   \n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 정제하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is practice sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       \n",
    "  \n",
    "    \n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  \n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  \n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'  \n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_####is ;;;^^^practice        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> baby it was all a dream <end>',\n",
       " '<start> i used to read word up magazine <end>',\n",
       " '<start> salt n pepa and heavy d up in the limousine <end>',\n",
       " '<start> hangin pictures on my wall <end>',\n",
       " '<start> every saturday rap attack mr magic marley marl <end>',\n",
       " '<start> i let my tape rock til my tape popped <end>',\n",
       " '<start> smokin weed and bambu sippin on private stock <end>',\n",
       " '<start> way back when i had the red and black lumberjack <end>',\n",
       " '<start> with the hat to match <end>',\n",
       " '<start> remember rappin duke duhha duhha <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 정제하기\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    # if len(processed.split()) <= 15: continue\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156013\n"
     ]
    }
   ],
   "source": [
    "# 토큰 개수가 15개를 넘어가는 문장 제외시키기\n",
    "processed = []\n",
    "\n",
    "for i in corpus:\n",
    "    if len(i.split()) <= 15:\n",
    "        processed.append(i)\n",
    "        \n",
    "print (len(processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   52   11 ...    0    0    0]\n",
      " [   2    4  285 ...    0    0    0]\n",
      " [   2 2876  480 ...    0    0    0]\n",
      " ...\n",
      " [   2    6  460 ...   26  205    3]\n",
      " [   2    8   42 ...    0    0    0]\n",
      " [   2    4   92 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fd59afe9050>\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 tensor로 변환\n",
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,  # 전체 단어갯수 \n",
    "        filters=' ',    \n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지 정함\n",
    "    )\n",
    "    tokenizer.fit_on_texts(processed)   # 구축한 corpus로부터 Tokenizer가 사전 자동구축\n",
    "\n",
    "    # tokenizer를 활용, 모델에 입력할 데이터셋 구축\n",
    "    tensor = tokenizer.texts_to_sequences(processed)   # 구축한 사전으로부터 corpus를 해석해 Tensor로 변환\n",
    "\n",
    "    # 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen = 15)  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n",
      "11 : it\n",
      "12 : me\n",
      "13 : my\n",
      "14 : in\n",
      "15 : that\n",
      "16 : t\n",
      "17 : s\n",
      "18 : on\n",
      "19 : your\n",
      "20 : of\n",
      "21 : we\n",
      "22 : .\n",
      "23 : like\n",
      "24 : m\n",
      "25 : all\n"
     ]
    }
   ],
   "source": [
    "# 구축된 단어사전 확인\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 25: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  52  11  53  25   9 361   3   0   0   0   0   0   0]\n",
      "[ 52  11  53  25   9 361   3   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# 소스와 타깃으로 분리\n",
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내 소스 문장 생성. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내 타겟 문장 생성\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_data = src_input\n",
    "txt_label = tgt_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import nltk\n",
    "with open(txt_data) as fp:\n",
    "    tokenized_text = [word for word in nltk.tokenize.word_tokenize(fp.read()) if len(word) <= 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124810, 14)\n",
      "Target Train: (124810, 14)\n"
     ]
    }
   ],
   "source": [
    "# train, test 데이터 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(txt_data,\n",
    "                                                          txt_label,\n",
    "                                                          test_size=0.2,\n",
    "                                                          random_state=7)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 객체 생성\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 인공지능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[-8.70321965e-05,  1.62195458e-04, -9.12767136e-05, ...,\n",
       "          2.96564132e-04,  9.78723328e-05, -1.37092429e-04],\n",
       "        [-1.09071538e-04,  2.83006229e-04, -4.64774203e-04, ...,\n",
       "          4.25667909e-04,  1.80758187e-04, -2.60813191e-04],\n",
       "        [-5.11544276e-05,  4.37017297e-04, -7.10082822e-04, ...,\n",
       "          5.97222417e-04,  8.68868228e-05, -2.75154714e-04],\n",
       "        ...,\n",
       "        [-5.40219422e-04, -5.66189818e-04,  3.86095257e-04, ...,\n",
       "         -3.23659205e-03, -1.98685168e-03, -1.24699273e-03],\n",
       "        [-5.71329263e-04, -5.89624222e-04,  4.55118163e-04, ...,\n",
       "         -3.60888918e-03, -2.06034631e-03, -1.34392164e-03],\n",
       "        [-5.88929048e-04, -6.04098605e-04,  4.98374575e-04, ...,\n",
       "         -3.94509081e-03, -2.10824842e-03, -1.41808251e-03]],\n",
       "\n",
       "       [[-8.70321965e-05,  1.62195458e-04, -9.12767136e-05, ...,\n",
       "          2.96564132e-04,  9.78723328e-05, -1.37092429e-04],\n",
       "        [-1.59556599e-04,  4.60096373e-04, -7.65513032e-05, ...,\n",
       "          5.18976594e-04,  1.92511929e-04, -2.78846710e-04],\n",
       "        [-9.14657503e-05,  5.52483252e-04,  8.41622459e-05, ...,\n",
       "          5.10859070e-04,  1.23911421e-04, -4.10419831e-04],\n",
       "        ...,\n",
       "        [-6.77471922e-04,  1.52965913e-05,  5.36308566e-04, ...,\n",
       "          6.56324439e-04, -5.73819678e-04, -4.96674620e-04],\n",
       "        [-2.51664460e-04, -3.62081337e-04,  6.08296250e-04, ...,\n",
       "          6.79546501e-04, -4.54657973e-04, -3.25830188e-04],\n",
       "        [-4.23403872e-05, -6.15848054e-04,  6.68110908e-04, ...,\n",
       "          5.18374902e-04, -7.16301613e-04, -5.37689848e-05]],\n",
       "\n",
       "       [[-8.70321965e-05,  1.62195458e-04, -9.12767136e-05, ...,\n",
       "          2.96564132e-04,  9.78723328e-05, -1.37092429e-04],\n",
       "        [-3.13089258e-04,  3.32612690e-04, -1.72403918e-04, ...,\n",
       "          4.13737056e-04, -1.82328877e-04, -2.85266637e-04],\n",
       "        [-6.17528160e-04,  3.96942836e-04, -5.19420646e-05, ...,\n",
       "          5.56074549e-04, -3.21845000e-04, -4.87647776e-04],\n",
       "        ...,\n",
       "        [-5.28023869e-04, -4.22790734e-04,  9.96850547e-04, ...,\n",
       "         -3.14228982e-03, -2.26768851e-03, -1.55977067e-03],\n",
       "        [-5.25670999e-04, -4.71601146e-04,  9.52778559e-04, ...,\n",
       "         -3.54835298e-03, -2.34255218e-03, -1.65063853e-03],\n",
       "        [-5.18581248e-04, -5.08229830e-04,  9.00435145e-04, ...,\n",
       "         -3.91384633e-03, -2.38574110e-03, -1.71045063e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-8.70321965e-05,  1.62195458e-04, -9.12767136e-05, ...,\n",
       "          2.96564132e-04,  9.78723328e-05, -1.37092429e-04],\n",
       "        [-1.33334615e-04,  1.88787570e-04, -5.09477759e-05, ...,\n",
       "          3.98697128e-04,  1.31249661e-04, -2.67157622e-04],\n",
       "        [-1.34965769e-04,  1.37388226e-04,  3.72383038e-05, ...,\n",
       "          3.05196882e-04,  2.80224805e-04, -2.68602220e-04],\n",
       "        ...,\n",
       "        [-6.44356303e-04, -4.65167512e-04,  7.91269238e-04, ...,\n",
       "         -1.28131616e-03, -9.67757427e-04, -3.38228827e-04],\n",
       "        [-6.60224643e-04, -5.40338748e-04,  8.30374425e-04, ...,\n",
       "         -1.81363139e-03, -1.26176164e-03, -6.54280768e-04],\n",
       "        [-6.64842897e-04, -6.00601255e-04,  8.34142265e-04, ...,\n",
       "         -2.30726902e-03, -1.51413365e-03, -9.24391381e-04]],\n",
       "\n",
       "       [[-8.70321965e-05,  1.62195458e-04, -9.12767136e-05, ...,\n",
       "          2.96564132e-04,  9.78723328e-05, -1.37092429e-04],\n",
       "        [-2.25572265e-04,  2.09841717e-04, -3.51003371e-04, ...,\n",
       "          3.39383900e-04, -1.12770445e-04, -1.52856228e-04],\n",
       "        [-2.82915455e-04,  4.46621561e-04, -6.33768155e-04, ...,\n",
       "          3.29853588e-04, -2.26096352e-04, -3.31665542e-05],\n",
       "        ...,\n",
       "        [-8.50368175e-04, -4.37552953e-05,  7.89957587e-04, ...,\n",
       "         -2.93812016e-03, -1.84785482e-03, -7.69034959e-04],\n",
       "        [-8.01359187e-04, -1.63022050e-04,  7.78034271e-04, ...,\n",
       "         -3.39525007e-03, -1.99263054e-03, -9.62075370e-04],\n",
       "        [-7.51480402e-04, -2.58676446e-04,  7.51376443e-04, ...,\n",
       "         -3.80181940e-03, -2.10311473e-03, -1.11695880e-03]],\n",
       "\n",
       "       [[-8.70321965e-05,  1.62195458e-04, -9.12767136e-05, ...,\n",
       "          2.96564132e-04,  9.78723328e-05, -1.37092429e-04],\n",
       "        [-1.57692557e-04,  1.93496118e-04, -3.30902461e-04, ...,\n",
       "          2.08063895e-04,  1.23802558e-04,  2.89635063e-05],\n",
       "        [-2.31460348e-04,  1.38326417e-04, -3.68343666e-04, ...,\n",
       "          1.26522049e-04,  2.79428234e-04, -2.46011245e-04],\n",
       "        ...,\n",
       "        [-7.79932816e-06,  5.82715322e-04,  1.08453305e-03, ...,\n",
       "         -1.51471572e-03, -2.05806387e-03,  1.09915665e-04],\n",
       "        [-1.85147277e-04,  4.50940453e-04,  1.26801396e-03, ...,\n",
       "         -2.01880420e-03, -2.27065780e-03, -2.67660653e-04],\n",
       "        [-3.24279477e-04,  3.07662645e-04,  1.37316959e-03, ...,\n",
       "         -2.50601931e-03, -2.42326292e-03, -6.31503644e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "609/609 [==============================] - 94s 155ms/step - loss: 3.4012\n",
      "Epoch 2/10\n",
      "609/609 [==============================] - 99s 163ms/step - loss: 2.9595\n",
      "Epoch 3/10\n",
      "609/609 [==============================] - 99s 163ms/step - loss: 2.7842\n",
      "Epoch 4/10\n",
      "609/609 [==============================] - 101s 165ms/step - loss: 2.6503\n",
      "Epoch 5/10\n",
      "609/609 [==============================] - 102s 167ms/step - loss: 2.5380\n",
      "Epoch 6/10\n",
      "609/609 [==============================] - 100s 164ms/step - loss: 2.4372\n",
      "Epoch 7/10\n",
      "609/609 [==============================] - 96s 157ms/step - loss: 2.3453\n",
      "Epoch 8/10\n",
      "609/609 [==============================] - 95s 155ms/step - loss: 2.2599\n",
      "Epoch 9/10\n",
      "609/609 [==============================] - 96s 158ms/step - loss: 2.1803\n",
      "Epoch 10/10\n",
      "609/609 [==============================] - 95s 155ms/step - loss: 2.1047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd597f7fc10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습시키기\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976/976 - 10s - loss: 2.0205\n",
      "2.0204739570617676\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "results = model.evaluate(enc_val, dec_val, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "   \n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "     \n",
    "    while True:\n",
    "        predict = model(test_tensor)   \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]    \n",
    "\n",
    "         \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        \n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i m the one that s gon hold you down <end> '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

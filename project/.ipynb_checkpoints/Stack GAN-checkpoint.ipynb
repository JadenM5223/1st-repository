{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pretty-metadata",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchfile\n",
      "  Downloading torchfile-0.1.0.tar.gz (5.2 kB)\n",
      "Building wheels for collected packages: torchfile\n",
      "  Building wheel for torchfile (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torchfile: filename=torchfile-0.1.0-py3-none-any.whl size=5711 sha256=5f1210cd8a013008b5b62ccdae40e8415c76b76fa532e1d60f81e8d234e4e816\n",
      "  Stored in directory: /home/aiffel/.cache/pip/wheels/ac/5c/3a/a80e1c65880945c71fd833408cd1e9a8cb7e2f8f37620bb75b\n",
      "Successfully built torchfile\n",
      "Installing collected packages: torchfile\n",
      "Successfully installed torchfile-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "spanish-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorboard import summary\n",
    "from six.moves import range\n",
    "from PIL import Image\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from copy import deepcopy\n",
    "from torch.nn import init\n",
    "\n",
    "#!pip install torchfile\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "previous-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------  dataset.py --------------------------------------------------\n",
    "\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class TextDataset(data.Dataset):\n",
    "    def __init__(self, data_dir, split='train', embedding_type='cnn-rnn',\n",
    "                 imsize=64, transform=None, target_transform=None):\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.imsize = imsize\n",
    "        self.data = []\n",
    "        self.data_dir = data_dir\n",
    "        if data_dir.find('birds') != -1:\n",
    "            self.bbox = self.load_bbox()\n",
    "        else:\n",
    "            print('checking...')\n",
    "            self.bbox = None\n",
    "        split_dir = os.path.join(data_dir, split)\n",
    "\n",
    "        self.filenames = self.load_filenames(split_dir)\n",
    "        self.embeddings = self.load_embedding(split_dir, embedding_type)\n",
    "        #self.class_id = self.load_class_id(split_dir, len(self.filenames))\n",
    "        # self.captions = self.load_all_captions()\n",
    "\n",
    "    def get_img(self, img_path, bbox):\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        width, height = img.size\n",
    "        if bbox is not None:\n",
    "            R = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
    "            center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
    "            center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
    "            y1 = np.maximum(0, center_y - R)\n",
    "            y2 = np.minimum(height, center_y + R)\n",
    "            x1 = np.maximum(0, center_x - R)\n",
    "            x2 = np.minimum(width, center_x + R)\n",
    "            img = img.crop([x1, y1, x2, y2])\n",
    "        load_size = int(self.imsize * 76 / 64)\n",
    "        img = img.resize((load_size, load_size), PIL.Image.BILINEAR)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    def load_bbox(self):\n",
    "        data_dir = self.data_dir\n",
    "        bbox_path = os.path.join(data_dir, 'CUB_200_2011/bounding_boxes.txt')\n",
    "        df_bounding_boxes = pd.read_csv(bbox_path,\n",
    "                                        delim_whitespace=True,\n",
    "                                        header=None).astype(int)\n",
    "        #\n",
    "        filepath = os.path.join(data_dir, 'CUB_200_2011/images.txt')\n",
    "        df_filenames = \\\n",
    "            pd.read_csv(filepath, delim_whitespace=True, header=None)\n",
    "        filenames = df_filenames[1].tolist()\n",
    "        print('Total filenames: ', len(filenames), filenames[0])\n",
    "        #\n",
    "        filename_bbox = {img_file[:-4]: [] for img_file in filenames}\n",
    "        numImgs = len(filenames)\n",
    "        for i in xrange(0, numImgs):\n",
    "            # bbox = [x-left, y-top, width, height]\n",
    "            bbox = df_bounding_boxes.iloc[i][1:].tolist()\n",
    "\n",
    "            key = filenames[i][:-4]\n",
    "            filename_bbox[key] = bbox\n",
    "        #\n",
    "        return filename_bbox\n",
    "\n",
    "    def load_all_captions(self):\n",
    "        caption_dict = {}\n",
    "        for key in self.filenames:\n",
    "            caption_name = '%s/text/%s.txt' % (self.data_dir, key)\n",
    "            captions = self.load_captions(caption_name)\n",
    "            caption_dict[key] = captions\n",
    "        return caption_dict\n",
    "\n",
    "    def load_captions(self, caption_name):\n",
    "        cap_path = caption_name\n",
    "        with open(cap_path, \"r\") as f:\n",
    "            captions = f.read().decode('utf8').split('\\n')\n",
    "        captions = [cap.replace(\"\\ufffd\\ufffd\", \" \")\n",
    "                    for cap in captions if len(cap) > 0]\n",
    "        return captions\n",
    "\n",
    "    def load_embedding(self, data_dir, embedding_type):\n",
    "        if embedding_type == 'cnn-rnn':\n",
    "            print('embedding....')\n",
    "            embedding_filename = os.path.join(data_dir, 'char-CNN-RNN-embeddings.pickle')\n",
    "            #print(embedding_filename)\n",
    "            #embedding_filename = '/char-CNN-RNN-embeddings.pickle'\n",
    "        elif embedding_type == 'cnn-gru':\n",
    "            embedding_filename = '/char-CNN-GRU-embeddings.pickle'\n",
    "        elif embedding_type == 'skip-thought':\n",
    "            embedding_filename = '/skip-thought-embeddings.pickle'\n",
    "\n",
    "        with open(embedding_filename, 'rb') as f:\n",
    "            #embeddings = pickle.load(f)\n",
    "            embeddings = pickle._Unpickler(f)\n",
    "            embeddings.encoding = 'latin1'\n",
    "            embeddings = embeddings.load()\n",
    "            #print(embeddings)\n",
    "            embeddings = np.array(embeddings)\n",
    "            # embedding_shape = [embeddings.shape[-1]]\n",
    "        '''with open('mnist.pkl', 'rb') as f:\n",
    "            u = pickle._Unpickler(f)\n",
    "            u.encoding = 'latin1'\n",
    "            p = u.load()'''\n",
    "            #print('embeddings: ', embeddings.shape)\n",
    "        return embeddings\n",
    "\n",
    "    '''def load_class_id(self, data_dir, total_num):\n",
    "        if os.path.isfile(data_dir + '/class_info.pickle'):\n",
    "            with open(data_dir + '/class_info.pickle', 'rb') as f:\n",
    "                class_id = pickle.load(f)\n",
    "        else:\n",
    "            class_id = np.arange(total_num)\n",
    "        return class_id'''\n",
    "    \n",
    "\n",
    "    def load_filenames(self, data_dir):\n",
    "        #../input/coco-data/coco/coco/train/filenames.pickle\n",
    "        filepath = os.path.join(data_dir, 'filenames.pickle')\n",
    "        with open(filepath, 'rb') as f:\n",
    "            filenames = pickle.load(f)\n",
    "        print('Load filenames from: %s (%d)' % (filepath, len(filenames)))\n",
    "        print(len(filenames))\n",
    "        print(args.IMG_DIR)\n",
    "        l=os.listdir('../input/coco-train-val2017/train2014/train2014')\n",
    "        im_file=[x.split('.')[0] for x in l]\n",
    "        #im_file = ['.'.join(x.split('.')[:-1]) for x in os.listdir(\"args.IMG_DIR\") if os.path.isfile(os.path.join('args.IMG_DIR', x))]\n",
    "        print(len(im_file))\n",
    "        #str = 'COCO_train2014_'\n",
    "        #im_file = [str + x for x in im_file]\n",
    "        print(im_file[0]) \n",
    "        print(filenames[0]) #COCO_train2014_\n",
    "        te = list(set(im_file) & set(filenames)) \n",
    "        print('te: ', len(te))\n",
    "        #list_ = ['.'.join(x.split('.')[:-1]) for x in os.listdir(\"path/to/Data\") if os.path.isfile(os.path.join('path/to/Data', x))]\n",
    "        #print(ssd)\n",
    "        return filenames\n",
    "\n",
    "    \n",
    "    '''def load_filenames(self, data_dir):\n",
    "        #../input/coco-data/coco/coco/train/filenames.pickle\n",
    "        filepath = os.path.join(data_dir, 'filenames.pickle')\n",
    "        with open(filepath, 'rb') as f:\n",
    "            filenames = pickle.load(f)\n",
    "        print('Load filenames from: %s (%d)' % (filepath, len(filenames)))\n",
    "        return filenames'''\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        key = self.filenames[index]\n",
    "        # cls_id = self.class_id[index]\n",
    "        #\n",
    "        if self.bbox is not None:\n",
    "            bbox = self.bbox[key]\n",
    "            data_dir = '%s/CUB_200_2011' % self.data_dir\n",
    "        else:\n",
    "            bbox = None\n",
    "            data_dir = self.data_dir\n",
    "\n",
    "        # captions = self.captions[key]\n",
    "        embeddings = self.embeddings[index, :, :]\n",
    "        #print(args.IMG_DIR)\n",
    "        #print(key)\n",
    "        #y = key[-12:]\n",
    "        img_name = '%s/%s.jpg' % (args.IMG_DIR, key)\n",
    "        #img_name = '%s/images/%s.jpg' % (data_dir, key)\n",
    "        img = self.get_img(img_name, bbox)\n",
    "\n",
    "        embedding_ix = random.randint(0, embeddings.shape[0]-1)\n",
    "        embedding = embeddings[embedding_ix, :]\n",
    "        if self.target_transform is not None:\n",
    "            embedding = self.target_transform(embedding)\n",
    "        return img, embedding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "distributed-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "#############################\n",
    "def KL_loss(mu, logvar):\n",
    "    # -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.mean(KLD_element).mul_(-0.5)\n",
    "    return KLD\n",
    "\n",
    "\n",
    "def compute_discriminator_loss(netD, real_imgs, fake_imgs,\n",
    "                               real_labels, fake_labels,\n",
    "                               conditions, gpus):\n",
    "    criterion = nn.BCELoss()\n",
    "    batch_size = real_imgs.size(0)\n",
    "    cond = conditions.detach()\n",
    "    fake = fake_imgs.detach()\n",
    "    real_features = nn.parallel.data_parallel(netD, (real_imgs), gpus)\n",
    "    fake_features = nn.parallel.data_parallel(netD, (fake), gpus)\n",
    "    # real pairs\n",
    "    inputs = (real_features, cond)\n",
    "    real_logits = nn.parallel.data_parallel(netD.get_cond_logits, inputs, gpus)\n",
    "    errD_real = criterion(real_logits, real_labels)\n",
    "    # wrong pairs\n",
    "    inputs = (real_features[:(batch_size-1)], cond[1:])\n",
    "    wrong_logits = \\\n",
    "        nn.parallel.data_parallel(netD.get_cond_logits, inputs, gpus)\n",
    "    errD_wrong = criterion(wrong_logits, fake_labels[1:])\n",
    "    # fake pairs\n",
    "    inputs = (fake_features, cond)\n",
    "    fake_logits = nn.parallel.data_parallel(netD.get_cond_logits, inputs, gpus)\n",
    "    errD_fake = criterion(fake_logits, fake_labels)\n",
    "\n",
    "    if netD.get_uncond_logits is not None:\n",
    "        real_logits = nn.parallel.data_parallel(netD.get_uncond_logits, (real_features), gpus)\n",
    "        fake_logits = nn.parallel.data_parallel(netD.get_uncond_logits, (fake_features), gpus)\n",
    "        uncond_errD_real = criterion(real_logits, real_labels)\n",
    "        uncond_errD_fake = criterion(fake_logits, fake_labels)\n",
    "        #\n",
    "        errD = ((errD_real + uncond_errD_real) / 2. + (errD_fake + errD_wrong + uncond_errD_fake) / 3.)\n",
    "        errD_real = (errD_real + uncond_errD_real) / 2.\n",
    "        errD_fake = (errD_fake + uncond_errD_fake) / 2.\n",
    "    else:\n",
    "        errD = errD_real + (errD_fake + errD_wrong) * 0.5\n",
    "    return errD, errD_real.data, errD_wrong.data, errD_fake.data\n",
    "\n",
    "\n",
    "def compute_generator_loss(netD, fake_imgs, real_labels, conditions, gpus):\n",
    "    criterion = nn.BCELoss()\n",
    "    cond = conditions.detach()\n",
    "    fake_features = nn.parallel.data_parallel(netD, (fake_imgs), gpus)\n",
    "    # fake pairs\n",
    "    inputs = (fake_features, cond)\n",
    "    fake_logits = nn.parallel.data_parallel(netD.get_cond_logits, inputs, gpus)\n",
    "    errD_fake = criterion(fake_logits, real_labels)\n",
    "    if netD.get_uncond_logits is not None:\n",
    "        fake_logits = \\\n",
    "            nn.parallel.data_parallel(netD.get_uncond_logits,\n",
    "                                      (fake_features), gpus)\n",
    "        uncond_errD_fake = criterion(fake_logits, real_labels)\n",
    "        errD_fake += uncond_errD_fake\n",
    "    return errD_fake\n",
    "\n",
    "\n",
    "#############################\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.0)\n",
    "\n",
    "\n",
    "#############################\n",
    "def save_img_results(data_img, fake, epoch, image_dir):\n",
    "    num = args.VIS_COUNT\n",
    "    fake = fake[0:num]\n",
    "    # data_img is changed to [0,1]\n",
    "    if data_img is not None:\n",
    "        data_img = data_img[0:num]\n",
    "        vutils.save_image(data_img, '%s/real_samples.png' % image_dir, normalize=True)\n",
    "        # fake.data is still [-1, 1]\n",
    "        vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (image_dir, epoch), normalize=True)\n",
    "    else:\n",
    "        vutils.save_image(fake.data, '%s/lr_fake_samples_epoch_%03d.png' % (image_dir, epoch), normalize=True)\n",
    "\n",
    "\n",
    "def save_model(netG, netD, epoch, model_dir):\n",
    "    torch.save(\n",
    "        netG.state_dict(),\n",
    "        '%s/netG_epoch_%d.pth' % (model_dir, epoch))\n",
    "    torch.save(\n",
    "        netD.state_dict(),\n",
    "        '%s/netD_epoch_last.pth' % (model_dir))\n",
    "    print('Save G/D models')\n",
    "\n",
    "\n",
    "def mkdir_p(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc:  # Python >2.5\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "strategic-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------  model.py --------------------------------------------------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "# Upsale the spatial size by a factor of 2\n",
    "def upBlock(in_planes, out_planes):\n",
    "    block = nn.Sequential(\n",
    "        nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "        conv3x3(in_planes, out_planes),\n",
    "        nn.BatchNorm2d(out_planes),\n",
    "        nn.ReLU(True))\n",
    "    return block\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channel_num):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            conv3x3(channel_num, channel_num),\n",
    "            nn.BatchNorm2d(channel_num),\n",
    "            nn.ReLU(True),\n",
    "            conv3x3(channel_num, channel_num),\n",
    "            nn.BatchNorm2d(channel_num))\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.block(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "class CA_NET(nn.Module):\n",
    "    # some code is modified from vae examples\n",
    "    # (https://github.com/pytorch/examples/blob/master/vae/main.py)\n",
    "    def __init__(self):\n",
    "        super(CA_NET, self).__init__()\n",
    "        self.t_dim = args.DIMENSION\n",
    "        self.c_dim = args.CONDITION_DIM\n",
    "        self.fc = nn.Linear(self.t_dim, self.c_dim * 2, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def encode(self, text_embedding):\n",
    "        x = self.relu(self.fc(text_embedding))\n",
    "        mu = x[:, :self.c_dim]\n",
    "        logvar = x[:, self.c_dim:]\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if args.CUDA:\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def forward(self, text_embedding):\n",
    "        mu, logvar = self.encode(text_embedding)\n",
    "        c_code = self.reparametrize(mu, logvar)\n",
    "        return c_code, mu, logvar\n",
    "\n",
    "class D_GET_LOGITS(nn.Module):\n",
    "    def __init__(self, ndf, nef, bcondition=True):\n",
    "        super(D_GET_LOGITS, self).__init__()\n",
    "        self.df_dim = ndf\n",
    "        self.ef_dim = nef\n",
    "        self.bcondition = bcondition\n",
    "        if bcondition:\n",
    "            self.outlogits = nn.Sequential(\n",
    "                conv3x3(ndf * 8 + nef, ndf * 8),\n",
    "                nn.BatchNorm2d(ndf * 8),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=4),\n",
    "                nn.Sigmoid())\n",
    "        else:\n",
    "            self.outlogits = nn.Sequential(\n",
    "                nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=4),\n",
    "                nn.Sigmoid())\n",
    "\n",
    "    def forward(self, h_code, c_code=None):\n",
    "        # conditioning output\n",
    "        if self.bcondition and c_code is not None:\n",
    "            c_code = c_code.view(-1, self.ef_dim, 1, 1)\n",
    "            c_code = c_code.repeat(1, 1, 4, 4)\n",
    "            # state size (ngf+egf) x 4 x 4\n",
    "            h_c_code = torch.cat((h_code, c_code), 1)\n",
    "        else:\n",
    "            h_c_code = h_code\n",
    "\n",
    "        output = self.outlogits(h_c_code)\n",
    "        return output.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "atlantic-ecuador",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############# Networks for stageI GAN #############\n",
    "class STAGE1_G(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(STAGE1_G, self).__init__()\n",
    "        self.gf_dim = args.GF_DIM * 8\n",
    "        self.ef_dim = args.CONDITION_DIM\n",
    "        self.z_dim = args.Z_DIM\n",
    "        self.define_module()\n",
    "\n",
    "    def define_module(self):\n",
    "        ninput = self.z_dim + self.ef_dim\n",
    "        ngf = self.gf_dim\n",
    "        # TEXT.DIMENSION -> GAN.CONDITION_DIM\n",
    "        self.ca_net = CA_NET()\n",
    "\n",
    "        # -> ngf x 4 x 4\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(ninput, ngf * 4 * 4, bias=False),\n",
    "            nn.BatchNorm1d(ngf * 4 * 4),\n",
    "            nn.ReLU(True))\n",
    "\n",
    "        # ngf x 4 x 4 -> ngf/2 x 8 x 8\n",
    "        self.upsample1 = upBlock(ngf, ngf // 2)\n",
    "        # -> ngf/4 x 16 x 16\n",
    "        self.upsample2 = upBlock(ngf // 2, ngf // 4)\n",
    "        # -> ngf/8 x 32 x 32\n",
    "        self.upsample3 = upBlock(ngf // 4, ngf // 8)\n",
    "        # -> ngf/16 x 64 x 64\n",
    "        self.upsample4 = upBlock(ngf // 8, ngf // 16)\n",
    "        # -> 3 x 64 x 64\n",
    "        self.img = nn.Sequential(\n",
    "            conv3x3(ngf // 16, 3),\n",
    "            nn.Tanh())\n",
    "\n",
    "    def forward(self, text_embedding, noise):\n",
    "        c_code, mu, logvar = self.ca_net(text_embedding)\n",
    "        z_c_code = torch.cat((noise, c_code), 1)\n",
    "        h_code = self.fc(z_c_code)\n",
    "\n",
    "        h_code = h_code.view(-1, self.gf_dim, 4, 4)\n",
    "        h_code = self.upsample1(h_code)\n",
    "        h_code = self.upsample2(h_code)\n",
    "        h_code = self.upsample3(h_code)\n",
    "        h_code = self.upsample4(h_code)\n",
    "        # state size 3 x 64 x 64\n",
    "        fake_img = self.img(h_code)\n",
    "        return None, fake_img, mu, logvar\n",
    "\n",
    "\n",
    "class STAGE1_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(STAGE1_D, self).__init__()\n",
    "        self.df_dim = args.DF_DIM\n",
    "        self.ef_dim = args.CONDITION_DIM\n",
    "        self.define_module()\n",
    "\n",
    "    def define_module(self):\n",
    "        ndf, nef = self.df_dim, self.ef_dim\n",
    "        self.encode_img = nn.Sequential(\n",
    "            nn.Conv2d(3, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf*2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf*4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            # state size (ndf * 8) x 4 x 4)\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        self.get_cond_logits = D_GET_LOGITS(ndf, nef)\n",
    "        self.get_uncond_logits = None\n",
    "\n",
    "    def forward(self, image):\n",
    "        img_embedding = self.encode_img(image)\n",
    "\n",
    "        return img_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "timely-filename",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############# Networks for stageII GAN #############\n",
    "class STAGE2_G(nn.Module):\n",
    "    def __init__(self, STAGE1_G):\n",
    "        super(STAGE2_G, self).__init__()\n",
    "        self.gf_dim = args.GF_DIM\n",
    "        self.ef_dim = args.CONDITION_DIM\n",
    "        self.z_dim = args.Z_DIM\n",
    "        self.STAGE1_G = STAGE1_G\n",
    "        # fix parameters of stageI GAN\n",
    "        for param in self.STAGE1_G.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.define_module()\n",
    "\n",
    "    def _make_layer(self, block, channel_num):\n",
    "        layers = []\n",
    "        for i in range(args.R_NUM):\n",
    "            layers.append(block(channel_num))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def define_module(self):\n",
    "        ngf = self.gf_dim\n",
    "        # TEXT.DIMENSION -> GAN.CONDITION_DIM\n",
    "        self.ca_net = CA_NET()\n",
    "        # --> 4ngf x 16 x 16\n",
    "        self.encoder = nn.Sequential(\n",
    "            conv3x3(3, ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(ngf, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(ngf * 2, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True))\n",
    "        self.hr_joint = nn.Sequential(\n",
    "            conv3x3(self.ef_dim + ngf * 4, ngf * 4),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True))\n",
    "        self.residual = self._make_layer(ResBlock, ngf * 4)\n",
    "        # --> 2ngf x 32 x 32\n",
    "        self.upsample1 = upBlock(ngf * 4, ngf * 2)\n",
    "        # --> ngf x 64 x 64\n",
    "        self.upsample2 = upBlock(ngf * 2, ngf)\n",
    "        # --> ngf // 2 x 128 x 128\n",
    "        self.upsample3 = upBlock(ngf, ngf // 2)\n",
    "        # --> ngf // 4 x 256 x 256\n",
    "        self.upsample4 = upBlock(ngf // 2, ngf // 4)\n",
    "        # --> 3 x 256 x 256\n",
    "        self.img = nn.Sequential(\n",
    "            conv3x3(ngf // 4, 3),\n",
    "            nn.Tanh())\n",
    "\n",
    "    def forward(self, text_embedding, noise):\n",
    "        _, stage1_img, _, _ = self.STAGE1_G(text_embedding, noise)\n",
    "        stage1_img = stage1_img.detach()\n",
    "        encoded_img = self.encoder(stage1_img)\n",
    "\n",
    "        c_code, mu, logvar = self.ca_net(text_embedding)\n",
    "        c_code = c_code.view(-1, self.ef_dim, 1, 1)\n",
    "        c_code = c_code.repeat(1, 1, 16, 16)\n",
    "        i_c_code = torch.cat([encoded_img, c_code], 1)\n",
    "        h_code = self.hr_joint(i_c_code)\n",
    "        h_code = self.residual(h_code)\n",
    "\n",
    "        h_code = self.upsample1(h_code)\n",
    "        h_code = self.upsample2(h_code)\n",
    "        h_code = self.upsample3(h_code)\n",
    "        h_code = self.upsample4(h_code)\n",
    "\n",
    "        fake_img = self.img(h_code)\n",
    "        return stage1_img, fake_img, mu, logvar\n",
    "\n",
    "\n",
    "class STAGE2_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(STAGE2_D, self).__init__()\n",
    "        self.df_dim = args.DF_DIM\n",
    "        self.ef_dim = args.CONDITION_DIM\n",
    "        self.define_module()\n",
    "\n",
    "    def define_module(self):\n",
    "        ndf, nef = self.df_dim, self.ef_dim\n",
    "        self.encode_img = nn.Sequential(\n",
    "            nn.Conv2d(3, ndf, 4, 2, 1, bias=False),  # 128 * 128 * ndf\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # 64 * 64 * ndf * 2\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # 32 * 32 * ndf * 4\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # 16 * 16 * ndf * 8\n",
    "            nn.Conv2d(ndf * 8, ndf * 16, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # 8 * 8 * ndf * 16\n",
    "            nn.Conv2d(ndf * 16, ndf * 32, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # 4 * 4 * ndf * 32\n",
    "            conv3x3(ndf * 32, ndf * 16),\n",
    "            nn.BatchNorm2d(ndf * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),   # 4 * 4 * ndf * 16\n",
    "            conv3x3(ndf * 16, ndf * 8),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)   # 4 * 4 * ndf * 8\n",
    "        )\n",
    "\n",
    "        self.get_cond_logits = D_GET_LOGITS(ndf, nef, bcondition=True)\n",
    "        self.get_uncond_logits = D_GET_LOGITS(ndf, nef, bcondition=False)\n",
    "\n",
    "    def forward(self, image):\n",
    "        img_embedding = self.encode_img(image)\n",
    "\n",
    "        return img_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "prompt-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------  trainer.py --------------------------------------------------\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torchfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "class GANTrainer(object):\n",
    "    def __init__(self, output_dir):\n",
    "        if args.FLAG:\n",
    "            self.model_dir = os.path.join(output_dir, 'Model')\n",
    "            self.image_dir = os.path.join(output_dir, 'Image')\n",
    "            self.log_dir = os.path.join(output_dir, 'Log')\n",
    "            mkdir_p(self.model_dir)\n",
    "            mkdir_p(self.image_dir)\n",
    "            mkdir_p(self.log_dir)\n",
    "            #self.summary_writer = FileWriter(self.log_dir)\n",
    "\n",
    "        self.max_epoch = args.MAX_EPOCH\n",
    "        self.snapshot_interval = args.SNAPSHOT_INTERVAL\n",
    "\n",
    "        s_gpus = args.GPU_ID.split(',')\n",
    "        self.gpus = [int(ix) for ix in s_gpus]\n",
    "        self.num_gpus = len(self.gpus)\n",
    "        self.batch_size = args.BATCH_SIZE * self.num_gpus\n",
    "        self.output_dir = output_dir\n",
    "        #print(self.gpus[0])\n",
    "        #torch.cuda.set_device(self.gpus[0])\n",
    "        cudnn.benchmark = True\n",
    "        \n",
    "        # ############# For training stageI GAN #############\n",
    "    def load_network_stageI(self):\n",
    "        #from model import STAGE1_G, STAGE1_D\n",
    "        netG = STAGE1_G()\n",
    "        netG.apply(weights_init)\n",
    "        print(netG)\n",
    "        netD = STAGE1_D()\n",
    "        netD.apply(weights_init)\n",
    "        print(netD)\n",
    "        print('***********************************************************')\n",
    "\n",
    "        if args.NET_G != '':\n",
    "            #state_dict = torch.load(args.NET_G, map_location=lambda storage, loc: storage)\n",
    "            #netG.load_state_dict(state_dict)\n",
    "            print('generator 1')\n",
    "            print('Load from: ', args.NET_G)\n",
    "        if args.NET_D != '':\n",
    "            #state_dict = torch.load(args.NET_D,map_location=lambda storage, loc: storage)\n",
    "            #netD.load_state_dict(state_dict)\n",
    "            print('discriminator 1')\n",
    "            print('Load from: ', args.NET_D)\n",
    "        if args.CUDA:\n",
    "            netG.cuda()\n",
    "            netD.cuda()\n",
    "        return netG, netD\n",
    "        \n",
    "    def load_network_stageII(self):\n",
    "        #from model import STAGE1_G, STAGE2_G, STAGE2_D\n",
    "\n",
    "        Stage1_G = STAGE1_G()\n",
    "        netG = STAGE2_G(Stage1_G)\n",
    "        netG.apply(weights_init)\n",
    "        #print(netG)\n",
    "        if args.NET_G != '':\n",
    "            #state_dict = torch.load(args.NET_G,map_location=lambda storage, loc: storage)\n",
    "            #netG.load_state_dict(state_dict)\n",
    "            print('Load from: ', args.NET_G)\n",
    "        elif args.STAGE1_G != '':\n",
    "            #state_dict = torch.load(args.STAGE1_G,map_location=lambda storage, loc: storage)\n",
    "            #netG.STAGE1_G.load_state_dict(state_dict)\n",
    "            print('Load from: ', args.STAGE1_G)\n",
    "        else:\n",
    "            print(\"Please give the Stage1_G path\")\n",
    "            return\n",
    "\n",
    "        netD = STAGE2_D()\n",
    "        netD.apply(weights_init)\n",
    "        if args.NET_D != '':\n",
    "            #state_dict = torch.load(args.NET_D,map_location=lambda storage, loc: storage)\n",
    "            #netD.load_state_dict(state_dict)\n",
    "            print('Load from: ', args.NET_D)\n",
    "        #print(netD)\n",
    "\n",
    "        if args.CUDA:\n",
    "            netG.cuda()\n",
    "            netD.cuda()\n",
    "        return netG, netD\n",
    "    \n",
    "    def train(self, data_loader, stage=1):\n",
    "        if stage == 1:\n",
    "            netG, netD = self.load_network_stageI()\n",
    "        else:\n",
    "            netG, netD = self.load_network_stageII()\n",
    "\n",
    "        nz = args.Z_DIM\n",
    "        batch_size = self.batch_size\n",
    "        noise = Variable(torch.FloatTensor(batch_size, nz))\n",
    "        fixed_noise = Variable(torch.FloatTensor(batch_size, nz).normal_(0, 1),volatile=True)\n",
    "        real_labels = Variable(torch.FloatTensor(batch_size).fill_(1))\n",
    "        fake_labels = Variable(torch.FloatTensor(batch_size).fill_(0))\n",
    "        if args.CUDA:\n",
    "            noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "            real_labels, fake_labels = real_labels.cuda(), fake_labels.cuda()\n",
    "\n",
    "        generator_lr = args.GENERATOR_LR\n",
    "        discriminator_lr = args.DISCRIMINATOR_LR\n",
    "        lr_decay_step = args.LR_DECAY_EPOCH\n",
    "        optimizerD = optim.Adam(netD.parameters(), lr=args.DISCRIMINATOR_LR, betas=(0.5, 0.999))\n",
    "        netG_para = []\n",
    "        for p in netG.parameters():\n",
    "            if p.requires_grad:\n",
    "                netG_para.append(p)\n",
    "        optimizerG = optim.Adam(netG_para,lr=args.GENERATOR_LR,betas=(0.5, 0.999))\n",
    "        count = 0\n",
    "        c = 0\n",
    "        for epoch in range(self.max_epoch):\n",
    "            if c == 1:\n",
    "                break\n",
    "            start_t = time.time()\n",
    "            if epoch % lr_decay_step == 0 and epoch > 0:\n",
    "                generator_lr *= 0.5\n",
    "                for param_group in optimizerG.param_groups:\n",
    "                    param_group['lr'] = generator_lr\n",
    "                discriminator_lr *= 0.5\n",
    "                for param_group in optimizerD.param_groups:\n",
    "                    param_group['lr'] = discriminator_lr\n",
    "            br = 0\n",
    "            for i, data in enumerate(data_loader, 0):\n",
    "                if br==3:\n",
    "                    break\n",
    "                ######################################################\n",
    "                # (1) Prepare training data\n",
    "                ######################################################\n",
    "                print('data: -- ', data)\n",
    "                print('---------------------')\n",
    "                real_img_cpu, txt_embedding = data\n",
    "                #print('embedding:  ', txt_embedding)\n",
    "                real_imgs = Variable(real_img_cpu)\n",
    "                txt_embedding = Variable(txt_embedding)\n",
    "                #print('text_embedding:  ', txt_embedding)\n",
    "                if args.CUDA:\n",
    "                    real_imgs = real_imgs.cuda()\n",
    "                    txt_embedding = txt_embedding.cuda()\n",
    "\n",
    "                #######################################################\n",
    "                # (2) Generate fake images\n",
    "                ######################################################\n",
    "                noise.data.normal_(0, 1)\n",
    "                inputs = (txt_embedding, noise)\n",
    "                _, fake_imgs, mu, logvar = \\\n",
    "                    nn.parallel.data_parallel(netG, inputs, self.gpus)\n",
    "\n",
    "                ############################\n",
    "                # (3) Update D network\n",
    "                ###########################\n",
    "                netD.zero_grad()\n",
    "                errD, errD_real, errD_wrong, errD_fake = \\\n",
    "                    compute_discriminator_loss(netD, real_imgs, fake_imgs,\n",
    "                                               real_labels, fake_labels,\n",
    "                                               mu, self.gpus)\n",
    "                errD.backward()\n",
    "                optimizerD.step()\n",
    "                ############################\n",
    "                # (2) Update G network\n",
    "                ###########################\n",
    "                netG.zero_grad()\n",
    "                errG = compute_generator_loss(netD, fake_imgs,\n",
    "                                              real_labels, mu, self.gpus)\n",
    "                kl_loss = KL_loss(mu, logvar)\n",
    "                errG_total = errG + kl_loss * args.KL\n",
    "                errG_total.backward()\n",
    "                optimizerG.step()\n",
    "                \n",
    "                br = br+1\n",
    "                \n",
    "                count = count + 1\n",
    "                if i % 10 == 0:\n",
    "                    '''summary_D = summary.scalar('D_loss', errD.data)\n",
    "                    summary_D_r = summary.scalar('D_loss_real', errD_real)\n",
    "                    summary_D_w = summary.scalar('D_loss_wrong', errD_wrong)\n",
    "                    summary_D_f = summary.scalar('D_loss_fake', errD_fake)\n",
    "                    summary_G = summary.scalar('G_loss', errG.data)\n",
    "                    summary_KL = summary.scalar('KL_loss', kl_loss.data)\n",
    "\n",
    "                    self.summary_writer.add_summary(summary_D, count)\n",
    "                    self.summary_writer.add_summary(summary_D_r, count)\n",
    "                    self.summary_writer.add_summary(summary_D_w, count)\n",
    "                    self.summary_writer.add_summary(summary_D_f, count)\n",
    "                    self.summary_writer.add_summary(summary_G, count)\n",
    "                    self.summary_writer.add_summary(summary_KL, count)'''\n",
    "\n",
    "                    # save the image result for each epoch\n",
    "                    inputs = (txt_embedding, fixed_noise)\n",
    "                    lr_fake, fake, _, _ = nn.parallel.data_parallel(netG, inputs, self.gpus)\n",
    "                    print('real_img_cpu', real_img_cpu.shape)   # real_img_cpu torch.Size([128, 3, 64, 64])\n",
    "                    #print('text_embedding:  ', txt_embedding)\n",
    "                    print(type(real_img_cpu))\n",
    "                    print(real_img_cpu[0].shape)\n",
    "                    plt.imshow(real_img_cpu[0].permute(1, 2, 0))\n",
    "                    #plt.imshow(real_img_cpu[1].numpy().squeeze(), cmap='RGB')\n",
    "                    #plt.imshow(real_img_cpu) \n",
    "                    '''save_img_results(real_img_cpu, fake, epoch, self.image_dir)\n",
    "                    if lr_fake is not None:\n",
    "                        save_img_results(None, lr_fake, epoch, self.image_dir)'''\n",
    "                print('br', br)\n",
    "            end_t = time.time()\n",
    "            c = 1\n",
    "            print('''[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f Loss_KL: %.4f\n",
    "                     Loss_real: %.4f Loss_wrong:%.4f Loss_fake %.4f\n",
    "                     Total Time: %.2fsec\n",
    "                  '''\n",
    "                  % (epoch, self.max_epoch, i, len(data_loader),\n",
    "                     errD.data, errG.data, kl_loss.data,\n",
    "                     errD_real, errD_wrong, errD_fake, (end_t - start_t)))\n",
    "            if epoch % self.snapshot_interval == 0:\n",
    "                save_model(netG, netD, epoch, self.model_dir)\n",
    "        #\n",
    "        #save_model(netG, netD, self.max_epoch, self.model_dir)\n",
    "        #\n",
    "        #self.summary_writer.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ambient-apple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021_03_29_15_46_18\n",
      "tmp/\n",
      "checking...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/coco-data/coco/coco/train/filenames.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-86a762dbad29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m         dataset = TextDataset(args.DATA_DIR, 'train',\n\u001b[1;32m     85\u001b[0m                               \u001b[0mimsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMSIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                               transform=image_transform)\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         dataloader = torch.utils.data.DataLoader(\n",
      "\u001b[0;32m<ipython-input-5-c295e404bc65>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_dir, split, embedding_type, imsize, transform, target_transform)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0msplit_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_filenames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m#self.class_id = self.load_class_id(split_dir, len(self.filenames))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-c295e404bc65>\u001b[0m in \u001b[0;36mload_filenames\u001b[0;34m(self, data_dir)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m#../input/coco-data/coco/coco/train/filenames.pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'filenames.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Load filenames from: %s (%d)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/coco-data/coco/coco/train/filenames.pickle'"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------  main.py --------------------------------------------------\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import pprint\n",
    "import datetime\n",
    "import dateutil\n",
    "import dateutil.tz\n",
    "import torchfile\n",
    "    \n",
    "class Struct:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    params = dict()\n",
    "    # parameters\n",
    "    parser = argparse.ArgumentParser()\n",
    "    params = dict()\n",
    "    params['CONFIG_NAME']='stageI'\n",
    "    params['DATASET_NAME']='coco'\n",
    "    params['EMBEDDING_TYPE']='cnn-rnn'\n",
    "    params['GPU_ID']='0'\n",
    "    params['CUDA']='TRUE'\n",
    "    params['WORKERS']=4\n",
    "    params['NET_G']=''\n",
    "                    #../input/models/coco_netg_epoch_90/coco_netG_epoch_90.pth\n",
    "    params['NET_D']=''\n",
    "    #params['STAGE1_G']='../output/coco_stageI/Model/netG_epoch_120.pth'\n",
    "    params['DATA_DIR']='../input/coco-data/coco/coco'\n",
    "    params['IMG_DIR'] = '../input/coco-train-val2017/train2014/train2014'\n",
    "    params['VIS_COUNT']=64\n",
    "    params['Z_DIM']=100\n",
    "    params['IMSIZE']=64\n",
    "    params['STAGE']=1\n",
    "    #TRAIN = edict()\n",
    "    params['FLAG']='TRUE'\n",
    "    params['BATCH_SIZE']=128\n",
    "    params['MAX_EPOCH']=2\n",
    "    params['SNAPSHOT_INTERVAL']=1\n",
    "    #params['PRETAINED_MODEL']=''\n",
    "    #params['PRETRAINED_EPOCH']=600\n",
    "    params['LR_DECAY_EPOCH']=20\n",
    "    params['DISCRIMINATOR_LR']=0.0002\n",
    "    params['GENERATOR_LR']=0.0002\n",
    "    params['KL']=2.0\n",
    "    #gan\n",
    "    params['CONDITION_DIM']=128\n",
    "    params['DF_DIM']=96\n",
    "    params['GF_DIM']=192\n",
    "    #params['R_NUM']=2\n",
    "    #text\n",
    "    params['DIMENSION']=1024\n",
    "    #params['manualSeed']=random.randint(1, 10000)\n",
    "    args = Struct(**params) #Convert nested Python dict to object\n",
    "\n",
    "   # s = Struct(**params)\n",
    "   # config = s             # parser.parse_args()\n",
    "   # print(config)\n",
    "    manualSeed = random.randint(1, 10000)\n",
    "    random.seed(manualSeed)\n",
    "    torch.manual_seed(manualSeed)\n",
    "    torch.cuda.manual_seed_all(manualSeed)\n",
    "    now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
    "    timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
    "    print(timestamp)\n",
    "    \n",
    "    output_dir = 'tmp/'\n",
    "    #output_dir = '../output/%s_%s_%s' % (args.DATASET_NAME, args.CONFIG_NAME, timestamp)\n",
    "    print(output_dir)\n",
    "    num_gpu = len(args.GPU_ID.split(','))\n",
    "    if args.FLAG:\n",
    "        image_transform = transforms.Compose([\n",
    "            transforms.RandomCrop(args.IMSIZE),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        dataset = TextDataset(args.DATA_DIR, 'train',\n",
    "                              imsize=args.IMSIZE,\n",
    "                              transform=image_transform)\n",
    "        assert dataset\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=args.BATCH_SIZE * num_gpu,\n",
    "            drop_last=True, shuffle=True, num_workers=int(args.WORKERS))\n",
    "\n",
    "        algo = GANTrainer(output_dir)\n",
    "        algo.train(dataloader, args.STAGE)\n",
    "        \n",
    "#'../input/visda2018-validation/val2017/val2017/000000144280.jpg'\n",
    "#'../input/visda2018-validation/val2017/val2017/000000212226.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-spain",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
